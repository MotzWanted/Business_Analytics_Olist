{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from surprise import SVD\n",
    "from surprise.dataset import Reader, Dataset\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender systems \n",
    "\n",
    "#### Collaborative filtering \n",
    "\n",
    "Collaborative recommenders rely on data generated by users as they interact with items. \n",
    "\n",
    "Benefits \n",
    "\n",
    "- it is always “self-generating” — users create the data for you naturally as they interact with items. This can be a valuable data source, especially in cases where high-quality item features are not available or difficult to obtain. \n",
    "- Another benefit of collaborative filters is that it helps users discover new items that are outside the subspace defined by their historical profile.\n",
    "\n",
    "Drawbacks \n",
    "\n",
    "- the well-known cold start problem. \n",
    "- It is also difficult for collaborative filters to accurately recommend novel or niche items because these items typically do not have enough user-item interaction data.\n",
    "\n",
    "**Item-item** (http://www.diva-portal.org/smash/get/diva2:1111865/FULLTEXT01.pdf):\n",
    "\n",
    "Item based collaborative filtering was introduced 1998 by Amazon[6]. Unlike user based collaborative filtering, item based filtering looks at the similarity between different items, and does this by taking note of how many users that bought item X also bought item Y. If the correlation is high enough, a similarity can be presumed to exist between the two items, and they can be assumed to be similar to one another. Item Y will from there on be recommended to users who bought item X and vice versa.\n",
    "\n",
    "In item-item collaborative filtering, we provide a recommendation based on other items similar to ours. The **benefits** of it, compared to user-user collaborative filtering, is that we usually need much fewer similarity computations (in most cases, there are much more users in systems than items). The most **common pitfall** - the system can provide very obvious recommendations.\n",
    "\n",
    "**User-user** (http://www.diva-portal.org/smash/get/diva2:1111865/FULLTEXT01.pdf):\n",
    "\n",
    "The report is focusing on the “nearest neighbour” approach for recommendations, which looks at the users rating patterns and finds the “nearest neighbours”, i.e users with ratings similar to yours. The algorithm then proceeds to give you recommendations based on the ratings of these neighbours.\n",
    "\n",
    "In user-user collaborative filtering, we provide a recommendation based on tastes of other users similar to us. **The problem** with that algorithm is that we need a lot of information about other people to provide correct recommendations, but the main benefits are effectiveness and ability to provide new, unexpected, and, yet, good recommendations.\n",
    "\n",
    "### Content recommenders\n",
    "\n",
    "Content recommenders rely on item features to make recommendations. \n",
    "\n",
    "Benefits\n",
    "\n",
    "- Content filters tend to be more robust against popularity bias and the cold start problem. \n",
    "- They can easily recommend new or novel items based on niche tastes. \n",
    "\n",
    "Drawbacks\n",
    "\n",
    "- However, in an item-to-item recommender, content filters can only recommend items with features similar to the original item. \n",
    "- This limits the scope of recommendations, and can also result in surfacing items with low ratings.\n",
    "\n",
    "##### In this project\n",
    "\n",
    "Given our dataset and the features we have, we are not able to create a content-based filtering algorithm, as the algorithm would need more information. We would essentially need to know some specific attributes about every product, e.g. with movies we know whether they include themes such as: Baseball, Economics, etc. \n",
    "Furthermore, companies such as Amazon are using collaborative item-item based recommender systems (https://www.quora.com/What-algorithm-s-does-Amazon-use-in-their-recommendation-system), and this is also the one that makes most sense in our case, since we do not have a lot of information of about each individual (we do not have many purchases from each person). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv(\"../data/olist_order_reviews_dataset.csv\" ,sep=',')\n",
    "df_orders = pd.read_csv(\"../data/olist_orders_dataset.csv\" ,sep=',')\n",
    "df_items = pd.read_csv(\"../data/olist_order_items_dataset.csv\" ,sep=',')\n",
    "df_products = pd.read_csv(\"../data/olist_products_dataset.csv\" ,sep=',')\n",
    "df_customer = pd.read_csv(\"../data/olist_customers_dataset.csv\" ,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"data_tables.png\")\n",
    "#Overview of the different data tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data mapping: \n",
    "\n",
    "The information essentially needed, is a tables with customer ID's in rows and products in the columns, with reviews as values in the dataframe. This means that the three, customers, reviews and products are matched using order and order items respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short overview of the relevant tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_customer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Matching product ID's and product categories on the order items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, we have some duplicates of orderID's, since the customers who are ordering several products in one order, has different `order_item_id`. Thus at first, the table df_items is enriched with the `product_id` and `product_category_name` for every item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matching items and products to obtain the product category name:\n",
    "ordered_products = df_items.merge(df_products, on=\"product_id\", how= \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude irrelevant columns:\n",
    "cols= ordered_products.columns\n",
    "ordered_products = ordered_products[cols[0:8]]\n",
    "ordered_products = ordered_products.drop([\"shipping_limit_date\", \"price\", \"freight_value\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_products.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Matching the unique customer ID's with the orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lenth of orders: \", len(df_orders), \"Length of customers: \", len(df_customer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the tables, there are less `customer_unique_id` than `customer_id`, which indicates that some customers has several `customer_id`'s. The unique customer ID's are now matched with all the `order_id`'s, such that every unique customers orders can be reviewd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique customer ID's is matched witht the given order. The key is customer_id\n",
    "unique_customer_orders = df_orders.merge(df_customer, on=\"customer_id\", how= \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And then we are only taking relevant columns:\n",
    "cols = list(unique_customer_orders.columns[0:2])\n",
    "cols.append(\"customer_unique_id\")\n",
    "unique_customer_orders = unique_customer_orders[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_customer_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_customer_orders.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. The task is now to match the reviews on to every order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short description of the review data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, some order_id's have multiple reviews. Looking at one of the order_id's with three reviews, we see the following data-points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[df_reviews.order_id=='8e17072ec97ce29f0e1f111e598b0c85']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to filter these duplicate values out, such that we do not have several reviews for each order. The reason why this might be, is that one order_id can have several items. However, it is not possible to link the different review_id's to the order_item_id, which means that we have to use assumptions. \n",
    "\n",
    "The approach is to use the aggregate mean for each order_id to calculate the score for that particular order. This might give the best result for the entire order and the different items in that order.\n",
    "\n",
    "The review table looks like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_reviews.groupby(\"order_id\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now match these reviews to every order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_orders = unique_customer_orders.merge(reviews, on=\"order_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Lastly, the two datasets with orders and items can now be combined. \n",
    "\n",
    "It is then assumed, that each item in an order is reviewed as the mean score for that particular order. This is an assumption which is mentioned in step 3. The all the reviews are therefore merged in to all the items, such that we get a per-item review score for each unique customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ordered_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_review = ordered_products.merge(unique_orders, on=\"order_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the final data-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above item_review table, it is possible to make a table including per-customer and per-item reviews. However, a problem arise when trying to make a custumer-item-review table, since there are 32,951 unique products and 95,420 unique customers, which is too large to hold in memeory. Therefore, the number of purchases per product is sorted, and the products which has very few purchases are excluded. Arguably, these products might also be uncertain to recommend to others as they have not recieved a lot of reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_review.product_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_review.customer_unique_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe that will contain how many reviews each product has\n",
    "pp = item_review.groupby(\"product_id\")[\"review_score\"].count().sort_values()\n",
    "ppdf = pd.DataFrame(pp)\n",
    "ppdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe which only contain products that has more than 1 review \n",
    "ppdf_small = ppdf[ppdf.review_score>10]\n",
    "len(ppdf_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the products with only 1 review out if the main dataframe\n",
    "products = list(ppdf_small.index)\n",
    "item_review_s = item_review[item_review['product_id'].isin(products)]\n",
    "len(item_review_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_review_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of the customers has bought the same products several times, these reviews has to be transformed in to one metric. Therefore, identical purchases from the same customer is averaged into one review score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averaging identical purchases\n",
    "item_review_ss = item_review_s.groupby([\"customer_unique_id\", \"product_id\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a pivot table, which has unique customers in the rows, product id's in the columns and review score as\n",
    "# values\n",
    "dfr = item_review_ss.pivot(index= \"customer_unique_id\", columns= \"product_id\", values=\"review_score\")\n",
    "#dfr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-item based collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we start out simple using an item-item system, where we can recommend different users products, that they have not consumed yet. Lets look at the top 10 products to recommend the first user in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recommending top ten products that fits best to the first customer in the dataset\n",
    "customer = list(dfr.index)[0]\n",
    "products = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the correlation for between each item\n",
    "correlations = dfr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding products that the customer has not yet purchased\n",
    "_purchased = dfr.loc[customer]\n",
    "_purchased = _purchased[_purchased.isnull()]\n",
    "_purchased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product, review in _purchased.iteritems():\n",
    "    rating = 0\n",
    "    weights_sum = 0\n",
    "    neighbours_corr = correlations[product].sort_values(ascending=False)[1: products+1]\n",
    "    item_mean = dfr[product].mean()\n",
    "    neighbours_ratings = dfr[neighbours_corr.index].transpose()\n",
    "    neighbours_means = neighbours_ratings.mean(axis=1)\n",
    "    for neighbour_id, row in neighbours_ratings.iterrows():\n",
    "        if np.isnan(row[customer]): continue\n",
    "        rating += neighbours_corr[neighbour_id] * (row[customer] - neighbours_means[neighbour_id])\n",
    "        weights_sum += abs(neighbours_corr[neighbour_id])\n",
    "    if weights_sum > 0:\n",
    "        rating /= weights_sum\n",
    "    rating += item_mean\n",
    "    _purchased.at[product] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Top 10 recommended products for the customer \n",
    "_purchased.sort_values(ascending=False)[0:products]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to look at what product category we are recommending the user, which is gives some more context to the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_cat = item_review_s[item_review_s[\"product_id\"].isin(list(pd.DataFrame(_purchased.sort_values(ascending=False)\\\n",
    "                        [0:products]).index))].product_category_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying recommended categories:\n",
    "list(rec_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, some of the products is within the same product category, since we only get eight different categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD \n",
    "\n",
    "Since an item-item based collaborative filtering approach is relatively simple we are building a more complex model using an SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting out the data needed\n",
    "data = item_review_s[[\"customer_unique_id\",\"product_id\", \"review_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the dataframe is: \", data.shape, \" and the data includes\", \\\n",
    "      len(data.groupby([\"customer_unique_id\"]).count()), \" unique customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have so many unique customers we will take a small subset, and only look at the customers who has made at least two purchases. This will dramatically reduce the size of the dataset, but it will also make the predictions of higher quality, as we will have more data on each customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe that will contain how many reviews each product has\n",
    "pp = data.groupby(\"customer_unique_id\")[\"product_id\"].count().sort_values()\n",
    "ppdf = pd.DataFrame(pp)\n",
    "ppdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe which only contain products that has more than 1 review \n",
    "ppdf_small = ppdf[ppdf.product_id>3]\n",
    "len(ppdf_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the products with only 1 review out if the main dataframe\n",
    "products = list(ppdf_small.index)\n",
    "data = data[data['customer_unique_id'].isin(products)]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()\n",
    "data = data.drop([\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can split the data into test and training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into test/train by a 70% split\n",
    "train_ind, test_ind = [], []\n",
    "#\n",
    "for i, product_id in enumerate(data['product_id'].unique()):\n",
    "    rows = data[data['product_id'] == product_id]\n",
    "#    print(rows)\n",
    "    ind = rows.index[:2].values.tolist()\n",
    "#    print(ind)\n",
    "    train_ind += ind\n",
    "\n",
    "#\n",
    "c = 0.7\n",
    "b = len(train_ind) / len(data)\n",
    "a = (c - b) / (1 - b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ind = set(range(len(data)))\n",
    "not_used = list(all_ind - set(train_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_done = True\n",
    "np.random.seed(42)\n",
    "while not_done:\n",
    "    np.random.shuffle(not_used)\n",
    "    train_ind_ = train_ind + not_used[:int(a * len(not_used))]\n",
    "    df_train = data.loc[train_ind_]\n",
    "    print(data.nunique()['product_id'], df_train.nunique()['product_id'])\n",
    "    print(data.nunique()['customer_unique_id'], df_train.nunique()['customer_unique_id'])\n",
    "    if data.nunique()['product_id'] == df_train.nunique()['product_id'] and data.nunique()['customer_unique_id'] == df_train.nunique()['customer_unique_id']:\n",
    "        not_done = False\n",
    "        train_ind = train_ind_\n",
    "\n",
    "test_ind = list(all_ind - set(train_ind))\n",
    "train_ind = sorted(train_ind)\n",
    "test_ind = sorted(test_ind)\n",
    "print(len(train_ind)/len(all_ind))\n",
    "print(len(test_ind)/len(all_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the code above takes quite a while, the is saved for future runs\n",
    "np.savetxt('data/recommender_data_train.csv', train_ind, fmt=\"%d\")\n",
    "np.savetxt('data/recommender_data_test.csv', test_ind, fmt=\"%d\")\n",
    "\n",
    "#if we want to load the data later\n",
    "#train_ind = np.loadtxt('data/data_train.csv', dtype=int)\n",
    "#test_ind = np.loadtxt('data/data_test.csv', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the train and test data from the split made above\n",
    "df_train = data.iloc[train_ind]\n",
    "df_test = data.iloc[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining helper functions to calculate RMSE and create a boxplot for later exploraiton of the precision of the model\n",
    "def RMSE(y_true, y_pred):\n",
    "    return np.linalg.norm(y_true - y_pred) / np.sqrt(len(y_true))\n",
    "\n",
    "def MakeBoxplot(y_true, y_pred, title):\n",
    "    data = [y_pred[y_true == (x*0.5+0.5)] for x in range(10)]\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.boxplot(data)\n",
    "    min_a, max_a = 0., 5.5\n",
    "    plt.xlim((min_a, max_a))\n",
    "    plt.ylim((min_a, max_a))\n",
    "    plt.plot([min_a, max_a * 2], [min_a, max_a], ls='--', color='gray', linewidth=1.0)\n",
    "    plt.xticks(range(12), [x*0.5 for x in range(12)])\n",
    "    plt.xlabel('True Rating')\n",
    "    plt.ylabel('Predicted Rating')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining y_true from the test data\n",
    "y_true = df_test['review_score'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy baseline \n",
    "\n",
    "As a start, we are creating a dummy baseline, which are predicting the mean of all the reviews as a prediction for every review unknown to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the global mean of all the training reviews as prediction for the unknown test datapoints. \n",
    "global_mean = df_train['review_score'].mean()\n",
    "print(\"global_mean =\", global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the variable global_mean as prediction\n",
    "y_pred = []\n",
    "for i, row in df_test.iterrows():\n",
    "    y_pred.append(global_mean)\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the performance of the model\n",
    "error = RMSE(y_true, y_pred)\n",
    "print(\"RMSE =\", error)\n",
    "MakeBoxplot(y_true, y_pred, 'Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Beating the baseline model\n",
    "\n",
    "As the average review for all products might be a bad estimate for how a given customer might rate some product, we will try to beat this baseline. For this purpose, an SVD model is implemented to predict ratings of unrated products for customers. This will hopefully beat the baseline model but also the item-item based collaborative filtering model. However, results from the SVD can not be directly compared to the predictions of the item-item based recommendation system, as we did not split values into test/train values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the rating scale (from 0.5 to 5) and loading our dataset from a pandas dataframe. \n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data_surprise = Dataset.load_from_df(df_train, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using grid search to tune the hyperparameters of the model. Note also, that we are using 5 cross validations. \n",
    "param_grid = {\n",
    "    'n_epochs': [5, 10, 20], \n",
    "    'lr_all': [0.005, 0.05, 0.01],\n",
    "    'reg_all': [0.001, 0.01, 0.1], \n",
    "    'n_factors': [10, 25, 50, 100],\n",
    "    'biased': [True, False]\n",
    "}\n",
    "\n",
    "#Creating the grid and conducting grid-search\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "gs.fit(data_surprise)\n",
    "\n",
    "#Printing the best RMSE score and the best hyperparameters: \n",
    "print(gs.best_score['rmse'])\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the model, as the one having the bese RMSE\n",
    "model = gs.best_estimator['rmse']\n",
    "model.fit(data_surprise.build_full_trainset());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the test values for customer and product ID's\n",
    "customer_ids_true = df_test['customer_unique_id'].values\n",
    "product_ids_true = df_test['product_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting customer values: \n",
    "y_pred = []\n",
    "for customer_id, product_id in zip(customer_ids_true, product_ids_true):\n",
    "    r = model.predict(customer_id, product_id, verbose=False).est\n",
    "    y_pred.append(r)\n",
    "y_pred = np.array(y_pred)\n",
    "# performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = RMSE(y_true, y_pred)\n",
    "print(\"RMSE =\", error)\n",
    "MakeBoxplot(y_true, y_pred, 'Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to beat the SVD with a deep learning model\n",
    "\n",
    "#https://medium.com/@jdwittenauer/deep-learning-with-keras-recommender-systems-e7b99cb29929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = item_review_s[[\"customer_unique_id\",\"product_id\", \"review_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"userId\", \"movieId\", \"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ratings.groupby('userId')['rating'].count()\n",
    "top_users = g.sort_values(ascending=False)[:15]\n",
    "g = ratings.groupby('movieId')['rating'].count()\n",
    "top_movies = g.sort_values(ascending=False)[:15]\n",
    "top_r = ratings.join(top_users, rsuffix='_r', how='inner', on='userId')\n",
    "top_r = top_r.join(top_movies, rsuffix='_r', how='inner', on='movieId')\n",
    "pd.crosstab(top_r.userId, top_r.movieId, top_r.rating, aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_enc = LabelEncoder()\n",
    "ratings['user'] = user_enc.fit_transform(ratings['userId'].values)\n",
    "n_users = ratings['user'].nunique()\n",
    "item_enc = LabelEncoder()\n",
    "ratings['movie'] = item_enc.fit_transform(ratings['movieId'].values)\n",
    "n_movies = ratings['movie'].nunique()\n",
    "ratings['rating'] = ratings['rating'].values.astype(np.float32)\n",
    "min_rating = min(ratings['rating'])\n",
    "max_rating = max(ratings['rating'])\n",
    "print(\"Number of customers, products, their min rating and max rating: \", n_users, n_movies, min_rating, max_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ratings[['user', 'movie']].values\n",
    "y = ratings['rating'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 50\n",
    "X_train_array = [X_train[:, 0], X_train[:, 1]]\n",
    "X_test_array = [X_test[:, 0], X_test[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecommenderV1(n_users, n_movies, n_factors):\n",
    "    user = Input(shape=(1,))\n",
    "    u = Embedding(n_users, n_factors, embeddings_initializer='he_normal',\n",
    "                  embeddings_regularizer=l2(1e-6))(user)\n",
    "    u = Reshape((n_factors,))(u)\n",
    "    \n",
    "    movie = Input(shape=(1,))\n",
    "    m = Embedding(n_movies, n_factors, embeddings_initializer='he_normal',\n",
    "                  embeddings_regularizer=l2(1e-6))(movie)\n",
    "    m = Reshape((n_factors,))(m)\n",
    "    \n",
    "    x = Dot(axes=1)([u, m])\n",
    "    model = Model(inputs=[user, movie], outputs=x)\n",
    "    #opt = Adam(lr=0.05,beta_1=0.001)\n",
    "    opt = RMSprop(lr=0.001, rho= 0.9)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecommenderV1(n_users, n_movies, n_factors)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train_array, y=y_train, batch_size=64, epochs=15,\n",
    "                    verbose=1, validation_data=(X_test_array, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"loss\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trying an even more advanced model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Add, Activation, Lambda\n",
    "class EmbeddingLayer:\n",
    "    def __init__(self, n_items, n_factors):\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = Embedding(self.n_items, self.n_factors, embeddings_initializer='he_normal',\n",
    "                      embeddings_regularizer=l2(1e-6))(x)\n",
    "        x = Reshape((self.n_factors,))(x)\n",
    "        return x\n",
    "def RecommenderV2(n_users, n_movies, n_factors, min_rating, max_rating):\n",
    "    user = Input(shape=(1,))\n",
    "    u = EmbeddingLayer(n_users, n_factors)(user)\n",
    "    ub = EmbeddingLayer(n_users, 1)(user)\n",
    "    \n",
    "    movie = Input(shape=(1,))\n",
    "    m = EmbeddingLayer(n_movies, n_factors)(movie)\n",
    "    mb = EmbeddingLayer(n_movies, 1)(movie)\n",
    "    x = Dot(axes=1)([u, m])\n",
    "    x = Add()([x, ub, mb])\n",
    "    x = Activation('sigmoid')(x)\n",
    "    x = Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(x)\n",
    "    model = Model(inputs=[user, movie], outputs=x)\n",
    "    opt = Adam(lr=0.001)\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=opt)#, metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecommenderV2(n_users, n_movies, n_factors, min_rating, max_rating)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train_array, y=y_train, batch_size=64, epochs=15,\n",
    "                    verbose=1, validation_data=(X_test_array, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The RMSE of the deep learning model is therefore: \", history.history[\"loss\"][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
